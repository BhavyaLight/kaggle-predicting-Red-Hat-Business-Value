{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "#from xgboost import XGBClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Non feature\n",
    "NON_FEATURE=['activity_id','people_id','date','people_date']\n",
    "\n",
    "# Categorical data that is only label encoded\n",
    "CATEGORICAL_DATA = ['people_char_1', 'people_char_2','people_group_1',\n",
    "                    'people_char_3', 'people_char_4', 'people_char_5',\n",
    "                    'people_char_6', 'people_char_7', 'people_char_8',\n",
    "                    'people_char_9', 'activity_category',\n",
    "                    'char_1', 'char_2', 'char_3', 'char_4', 'char_5', 'char_6',\n",
    "                    'char_7', 'char_8', 'char_9', 'char_10']\n",
    "\n",
    "# Already in a one-hot encoded form\n",
    "CATEGORICAL_BINARY = ['people_char_10', 'people_char_11', 'people_char_12',\n",
    "                      'people_char_13', 'people_char_14', 'people_char_15',\n",
    "                      'people_char_16', 'people_char_17', 'people_char_18',\n",
    "                      'people_char_19', 'people_char_20', 'people_char_21',\n",
    "                      'people_char_22', 'people_char_23', 'people_char_24',\n",
    "                      'people_char_25', 'people_char_26', 'people_char_27',\n",
    "                      'people_char_28', 'people_char_29', 'people_char_30',\n",
    "                      'people_char_31', 'people_char_32', 'people_char_33',\n",
    "                      'people_char_34', 'people_char_35', 'people_char_36',\n",
    "                      'people_char_37' ]\n",
    "\n",
    "# Continuous categories\n",
    "CONT = ['people_days', 'days',\n",
    "        'people_month',  'month',\n",
    "        'people_quarter', 'quarter',\n",
    "        'people_week', 'week',\n",
    "        'people_dayOfMonth', 'dayOfMonth',\n",
    "        'people_year', 'year',\n",
    "        'people_char_38']\n",
    "\n",
    "\n",
    "# Path to people.csv from ReadHat Kaggle data set with reduced dimensions\n",
    "FEATURE_FILE ='Data/act_train_features_reduced.csv'\n",
    "# Path to act_train.csv from RedHat Kaggle data set with reduced dimensions\n",
    "OUTPUT ='Data/act_train_output.csv'\n",
    "\n",
    "TEST_FILE = 'Data/act_test_features_reduced.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to change labels of categories to one-hot encoding using scikit's OneHot Encoding\n",
    "# pd.get_dummies(df) does the same, provides sweet header's as well but it it not fast enough, kill's memory\n",
    "def category_to_one_hot(dataset, non_feature, continuous_feature):\n",
    "    # Function to change labels of categories to one-hot encoding using scikit's OneHot Encoding sparse matrix\n",
    "    # pd.get_dummies(df) does the same, provides sweet header's as well but it kill's memory\n",
    "    ds = dataset.drop(non_feature, axis=1)\n",
    "    boolean_column = []\n",
    "    counter = 0\n",
    "    for column in ds.columns:\n",
    "        if column not in continuous_feature:\n",
    "            boolean_column.append(counter)\n",
    "        counter += 1\n",
    "    # boolean_column is not the column name but index\n",
    "    print(\"Done filtering columns...\")\n",
    "    grd_enc = OneHotEncoder(categorical_features=boolean_column)\n",
    "    encoded_arr = grd_enc.fit_transform(ds)\n",
    "    return encoded_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read the data set. Note this dataset does not contain the 'outcome' columns\n",
    "train_data_df = pd.read_csv(FEATURE_FILE,parse_dates=[\"date\"])\n",
    "train_data_df.sort_values(by=['activity_id'],ascending=True, inplace=True)\n",
    "\n",
    "\n",
    "# Read the train data output\n",
    "train_output = pd.read_csv(OUTPUT)\n",
    "train_output.sort_values(by='activity_id',ascending=True, inplace=True)\n",
    "v_out=train_output['outcome'].as_matrix()\n",
    "\n",
    "test_data_df = pd.read_csv(TEST_FILE,parse_dates=[\"date\"])\n",
    "test_data_df.sort_values(by=['activity_id'],ascending=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#NON_FEATURES=[]\n",
    "#for column in data_df.columns:\n",
    "#    if column not in SELECTED_FEATURES:\n",
    "#        NON_FEATURES.append(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############################ UNTOUCHED ######################################\n",
    "def normalize(df,non_features):\n",
    "    df=df.drop(non_features,axis=1)\n",
    "    features=df.columns\n",
    "    # Normalize categorical values to range betwwen 0-1, time usually : 3secs\n",
    "    start=time.time()\n",
    "    scaler = Normalizer()\n",
    "    df[features] = scaler.fit_transform(df[features])\n",
    "    end = time.time()\n",
    "    print (end-start)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Try random forest + linear regression\n",
    "# forest_data=data_df[[ 'people_group_1']].as_matrix()\n",
    "#X_train, X_test, y_train, y_test = train_test_split(train_data_df,v_out, test_size=0.4, random_state=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done filtering columns...\n",
      "274.782145977\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "## SAMPLE: without dropping char_10\n",
    "train_arr = category_to_one_hot(train_data_df, NON_FEATURE, CONT)\n",
    "## SAMPLE: try to run with char_10 first, if it does crash, you add it in NON_FEATURE and then run this code. Okay?\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done filtering columns...\n",
      "15.4397270679\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "## SAMPLE: without dropping char_10\n",
    "test_arr = category_to_one_hot(test_data_df, NON_FEATURE, CONT)\n",
    "## SAMPLE: try to run with char_10 first, if it does crash, you add it in NON_FEATURE and then run this code. Okay?\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scal=Normalizer()\n",
    "arr_train_norm=scal.fit_transform(train_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scal=Normalizer()\n",
    "arr_test_norm=scal.fit_transform(test_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "xgb = XGBClassifier(max_depth=10, learning_rate=0.3, n_estimators=25,\n",
    "                    objective='binary:logistic', subsample=0.7,\n",
    "                    colsample_bytree=0.7, seed=0, silent=1, nthread=4,\n",
    "                    min_child_weight=0)\n",
    "\n",
    "\n",
    "\n",
    "watchlist  = [(arr_train_norm,'train')]\n",
    "num_round = 300\n",
    "early_stopping_rounds=10\n",
    "\n",
    "#bst = xgb.train(param, arr_train_norm, num_round, watchlist,early_stopping_rounds=early_stopping_rounds)\n",
    "\n",
    "xgb.fit(arr_train_norm, v_out,eval_metric='auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#y_pred = xgb.predict(arr_test_norm)\n",
    "#predictions = [round(value) for value in y_pred]\n",
    "\n",
    "#For predicting probability for final kaggle outcome\n",
    "predictions = bst.predict(arr_test_norm)\n",
    "#predictions = xgb.predict_proba(arr_test_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.savetxt('feature_importance_xgb' + '.txt',\n",
    "           xgb.feature_importances_, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data_df['outcome']=predictions[:,1]\n",
    "\n",
    "test_data_df[['outcome','activity_id']].set_index('activity_id').drop('act_0').to_csv(\"XGBOOST_results.csv\")\n",
    "\n",
    "\n",
    "#from sklearn.metrics import accuracy_score\n",
    "\n",
    "# evaluate predictions\n",
    "#accuracy = accuracy_score(y_test, y_pred)\n",
    "#print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
